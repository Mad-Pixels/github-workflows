---
name: 'Sync Directory to S3'
description: 'Upload a local directory to S3'

inputs:
  aws_access_key:
    description: 'AWS access key ID'
    required: false
  aws_secret_key:
    description: 'AWS secret access key'
    required: false
  role_to_assume:
    description: 'AWS IAM role ARN to assume'
    required: false
  aws_region:
    description: 'AWS region'
    required: true

  source_dir:
    description: 'Local path to sync'
    required: true
  bucket_name:
    description: 'Target S3 bucket name'
    required: true
  bucket_prefix:
    description: 'Optional subpath prefix inside the bucket'
    required: false
    default: ''

  delete_removed:
    description: 'Remove S3 files not in source_dir (true/false)'
    required: false
    default: 'true'
  exclude_patterns:
    description: 'Space-separated exclude patterns (e.g. ".git/* *.tmp")'
    required: false
    default: ".git/* .github/* .gitignore .gitattributes"
  cache_control:
    description: 'Value for Cache-Control header (applied to uploaded/updated files)'
    required: false
  content_type_detection:
    description: 'Enable automatic content-type detection based on file extensions'
    required: false
    default: 'true'

  show_summary:
    description: 'Print summary in the job summary'
    required: false
    default: 'true'
  summary_limit:
    description: 'Max number of output lines to show in summary (kept for consistency)'
    required: false
    default: '250'

outputs:
  files_uploaded:
    description: 'Number of uploaded files'
    value: ${{ steps.sync.outputs.files_uploaded }}
  files_deleted:
    description: 'Number of deleted files'
    value: ${{ steps.sync.outputs.files_deleted }}
  total_size:
    description: 'Total size of local source (bytes)'
    value: ${{ steps.analyze.outputs.total_size }}
  file_count:
    description: 'Total number of files in source directory'
    value: ${{ steps.analyze.outputs.file_count }}
  sync_duration:
    description: 'Sync duration in seconds'
    value: ${{ steps.sync.outputs.sync_duration }}
  s3_url:
    description: 'Final S3 sync URL'
    value: ${{ steps.url.outputs.s3_url }}

runs:
  using: composite
  steps:
    - name: Validate inputs
      shell: bash
      run: |
        set -euo pipefail

        BUCKET="${{ inputs.bucket_name }}"
        if [[ ${#BUCKET} -lt 3 || ${#BUCKET} -gt 63 ]]; then
          echo "‚ùå S3 bucket name must be 3-63 characters: $BUCKET"
          exit 1
        fi

        if [[ ! "$BUCKET" =~ ^[a-z0-9][a-z0-9.-]{1,61}[a-z0-9]$ ]]; then
          echo "‚ùå Invalid S3 bucket name: $BUCKET"
          echo "Must be lowercase letters/digits/dots/hyphens, not start/end with dot/hyphen"
          exit 1
        fi

        if [[ "$BUCKET" == *".."* ]]; then
          echo "‚ùå S3 bucket name cannot contain consecutive dots: $BUCKET"
          exit 1
        fi

        if [[ ! "${{ inputs.aws_region }}" =~ ^[a-z]{2}-[a-z]+-[0-9]+$ ]]; then
          echo "‚ùå Invalid AWS region: ${{ inputs.aws_region }}"
          exit 1
        fi

        if [[ ! -d "${{ inputs.source_dir }}" ]]; then
          echo "‚ùå Source directory not found: ${{ inputs.source_dir }}"
          exit 1
        fi

        if [[ -z "$(find "${{ inputs.source_dir }}" -type f -print -quit)" ]]; then
          echo "‚ùå Source directory is empty: ${{ inputs.source_dir }}"
          exit 1
        fi

        echo "‚úÖ Inputs validated"

    - name: Validate exclude patterns
      shell: bash
      run: |
        set -euo pipefail

        EXCLUDES="${{ inputs.exclude_patterns }}"
        if [[ -n "$EXCLUDES" ]]; then
          echo "üîç Validating exclude patterns..."
          read -r -a patterns <<< "$EXCLUDES"

          for pattern in "${patterns[@]}"; do
            [[ -z "$pattern" ]] && continue
            if [[ "$pattern" == "/*" || "$pattern" == "*" ]]; then
              echo "‚ö†Ô∏è Exclude pattern '$pattern' will exclude everything"
            fi
            echo "  - Exclude: '$pattern'"
          done
        fi

        echo "‚úÖ Exclude patterns validated"

    - name: Configure AWS authentication
      uses: Mad-Pixels/github-workflows/internal/aws-auth@v1
      with:
        aws_access_key: ${{ inputs.aws_access_key }}
        aws_secret_key: ${{ inputs.aws_secret_key }}
        role_to_assume: ${{ inputs.role_to_assume }}
        aws_region: ${{ inputs.aws_region }}

    - name: Build S3 URL
      id: url
      shell: bash
      run: |
        set -euo pipefail

        BUCKET="${{ inputs.bucket_name }}"
        PREFIX="${{ inputs.bucket_prefix }}"
        PREFIX="${PREFIX#/}"; PREFIX="${PREFIX%/}"

        [[ -n "$PREFIX" ]] && S3_URL="s3://$BUCKET/$PREFIX" || S3_URL="s3://$BUCKET"
        echo "s3_url=$S3_URL" >> "$GITHUB_OUTPUT"
        echo "S3_URL=$S3_URL"  >> "$GITHUB_ENV"

    - name: Check S3 bucket access
      shell: bash
      run: |
        set -euo pipefail

        if ! command -v aws >/dev/null 2>&1; then
          echo "‚ÑπÔ∏è AWS CLI not found; skipping pre-check (sync will fail if perms are wrong)."
          exit 0
        fi

        echo "üîç Checking S3 bucket access..."
        if ! aws s3api head-bucket \
          --bucket "${{ inputs.bucket_name }}" \
          --region "${{ inputs.aws_region }}" >/dev/null 2>&1; then

          echo "‚ùå Cannot access S3 bucket: ${{ inputs.bucket_name }}"
          echo "   Ensure the bucket exists, region matches, and IAM permissions are sufficient."
          exit 1
        fi

        echo "‚úÖ S3 bucket accessible"

    - name: Analyze source directory
      id: analyze
      shell: bash
      run: |
        set -euo pipefail

        SRC="${{ inputs.source_dir }}"
        echo "üìä Analyzing source directory: $SRC"

        FILE_COUNT=$(find "$SRC" -type f | wc -l | awk '{print $1}')
        if find "$SRC" -type f -printf '%s\n' >/dev/null 2>&1; then
          BYTES=$(find "$SRC" -type f -printf '%s\n' | awk '{sum+=$1} END{print sum+0}')
        else
          BYTES=$(find "$SRC" -type f -exec stat -f%z {} \; 2>/dev/null | awk '{sum+=$1} END{print sum+0}' || echo "0")
        fi

        echo "file_count=$FILE_COUNT" >> "$GITHUB_OUTPUT"
        echo "total_size=$BYTES" >> "$GITHUB_OUTPUT"

    - name: Sync files to S3
      id: sync
      shell: bash
      run: |
        set -euo pipefail

        SYNC_START=$(date +%s)
        SRC="${{ inputs.source_dir }}"
        REGION="${{ inputs.aws_region }}"
        DELETE="${{ inputs.delete_removed }}"
        EXCLUDES="${{ inputs.exclude_patterns }}"
        CACHE_CONTROL="${{ inputs.cache_control }}"
        DETECT_CT="${{ inputs.content_type_detection }}"
        S3_URL="${S3_URL:?missing}"

        SYNC_CMD=(aws s3 sync "$SRC" "$S3_URL" --region "$REGION" --no-progress)
        [[ "$DELETE" == "true" ]] && SYNC_CMD+=("--delete")
        [[ "$DETECT_CT" == "false" ]] && SYNC_CMD+=("--no-guess-mime-type")

        if [[ -n "$EXCLUDES" ]]; then
          read -r -a patterns <<< "$EXCLUDES"
          for pattern in "${patterns[@]}"; do
            [[ -n "$pattern" ]] && SYNC_CMD+=("--exclude" "$pattern")
          done
        fi

        [[ -n "$CACHE_CONTROL" ]] && SYNC_CMD+=("--cache-control" "$CACHE_CONTROL")
        echo "üîÑ Executing: ${SYNC_CMD[*]}"

        set +e
        OUTPUT="$("${SYNC_CMD[@]}" 2>&1)"
        EXIT_CODE=$?
        set -e

        echo "$OUTPUT"
        [[ $EXIT_CODE -ne 0 ]] && { echo "‚ùå Sync failed ($EXIT_CODE)"; exit 1; }

        SYNC_END=$(date +%s)
        SYNC_DURATION=$((SYNC_END - SYNC_START))

        FILES_UPLOADED=$(echo "$OUTPUT" | grep -E -c '^upload:' || true)
        FILES_DELETED=$(echo "$OUTPUT" | grep -E -c '^delete:' || true)

        echo "files_uploaded=$FILES_UPLOADED" >> "$GITHUB_OUTPUT"
        echo "files_deleted=$FILES_DELETED" >> "$GITHUB_OUTPUT"
        echo "sync_duration=$SYNC_DURATION" >> "$GITHUB_OUTPUT"

    - name: Summary
      if: always() && inputs.show_summary == 'true'
      continue-on-error: true
      shell: bash
      env:
        LIMIT: ${{ inputs.summary_limit }}
        OUTCOME: ${{ steps.sync.outcome }}
        BUCKET: ${{ inputs.bucket_name }}
        SOURCE_DIR: ${{ inputs.source_dir }}
        S3_URL: ${{ steps.url.outputs.s3_url }}
        AWS_REGION: ${{ inputs.aws_region }}
        DELETE_REMOVED: ${{ inputs.delete_removed }}
        CACHE_CONTROL: ${{ inputs.cache_control }}
        CT_DETECTION: ${{ inputs.content_type_detection }}
        EXCLUDES: ${{ inputs.exclude_patterns }}
        FILES_UPLOADED: ${{ steps.sync.outputs.files_uploaded }}
        FILES_DELETED: ${{ steps.sync.outputs.files_deleted }}
        FILE_COUNT: ${{ steps.analyze.outputs.file_count }}
        TOTAL_SIZE: ${{ steps.analyze.outputs.total_size }}
        SYNC_DURATION: ${{ steps.sync.outputs.sync_duration }}
      run: |
        set -eo pipefail
        [[ "$LIMIT" =~ ^[0-9]+$ ]] || LIMIT=250

        STATUS_ICON="‚ùå"
        [ "${OUTCOME:-failure}" = "success" ] && STATUS_ICON="‚úÖ"

        BYTES="${TOTAL_SIZE:-0}"
        if [[ -z "$BYTES" || ! "$BYTES" =~ ^[0-9]+$ ]]; then BYTES=0; fi
        MB=$(awk "BEGIN {printf \"%.2f\", ($BYTES)/1024/1024}")

        {
          echo "## ‚òÅÔ∏è S3 Sync ${STATUS_ICON}"
          echo "- **Bucket:** \`${BUCKET:-}\`"
          echo "- **Source:** \`${SOURCE_DIR:-}\`"
          echo "- **Target:** \`${S3_URL:-}\`"
          echo "- **Region:** \`${AWS_REGION:-}\`"
          echo "- **Delete removed:** \`${DELETE_REMOVED:-false}\`"
          echo "- **Cache-Control:** \`${CACHE_CONTROL:-N/A}\`"
          echo "- **Content-Type detection:** \`${CT_DETECTION:-true}\`"
          echo "- **Excludes:** \`${EXCLUDES:-}\`"

          if [ "${OUTCOME:-failure}" = "success" ]; then
            echo "- **Files uploaded:** \`${FILES_UPLOADED:-0}\`"
            echo "- **Files deleted:** \`${FILES_DELETED:-0}\`"
            echo "- **Total files:** \`${FILE_COUNT:-0}\`"
            echo "- **Total size:** \`${BYTES}\` bytes (~${MB} MiB)"
            echo "- **Sync duration:** \`${SYNC_DURATION:-N/A}\` seconds"
          else
            echo "- **Status:** Sync failed ‚Äî check logs for details"
          fi
        } >> "${GITHUB_STEP_SUMMARY:-/dev/null}" || true
